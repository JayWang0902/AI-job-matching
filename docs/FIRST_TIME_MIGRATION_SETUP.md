# ğŸš€ é¦–æ¬¡éƒ¨ç½²ç”Ÿæˆè¿ç§»è„šæœ¬æŒ‡å—

## é—®é¢˜

ä½ çš„é¡¹ç›®ç›®å‰ `alembic/versions/` ç›®å½•æ˜¯ç©ºçš„ï¼Œéœ€è¦ç”Ÿæˆåˆå§‹è¿ç§»è„šæœ¬ã€‚

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ³• 1: åœ¨ EC2 é¦–æ¬¡éƒ¨ç½²åç”Ÿæˆï¼ˆæ¨èï¼‰

```bash
# 1. SSH åˆ° EC2
ssh -i key.pem ubuntu@your-ec2-ip

# 2. è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/AI-job-matching

# 3. ç¡®ä¿ backend å®¹å™¨å·²å¯åŠ¨
docker compose ps

# 4. ç”Ÿæˆåˆå§‹è¿ç§»è„šæœ¬
docker compose exec backend alembic revision --autogenerate -m "Initial migration: create all tables"

# è¾“å‡ºï¼š
# INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
# INFO  [alembic.runtime.migration] Generating /app/alembic/versions/abc123_initial_migration_create_all_tables.py ...  done

# 5. æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
docker compose exec backend ls -la alembic/versions/

# 6. åº”ç”¨è¿ç§»ï¼ˆåˆ›å»ºè¡¨ï¼‰
docker compose exec backend alembic upgrade head

# 7. æäº¤åˆ° Gitï¼ˆé‡è¦ï¼ï¼‰
git add alembic/versions/
git commit -m "feat: add initial database migration"
git push origin main
```

### æ–¹æ³• 2: æœ¬åœ°ç”Ÿæˆï¼ˆå¦‚æœä½ æœ¬åœ°æœ‰ PostgreSQLï¼‰

```bash
# 1. ç¡®ä¿ .env ä¸­çš„ DATABASE_URL æ­£ç¡®
# DATABASE_URL=postgresql://username:password@localhost:5432/dbname

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source ai-job-matching/bin/activate

# 3. ç”Ÿæˆè¿ç§»
alembic revision --autogenerate -m "Initial migration: create all tables"

# 4. æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
ls -la alembic/versions/

# 5. æäº¤åˆ° Git
git add alembic/versions/
git commit -m "feat: add initial database migration"
git push origin main
```

### æ–¹æ³• 3: æ‰‹åŠ¨åˆ›å»ºè¿ç§»è„šæœ¬ï¼ˆå¤‡é€‰ï¼‰

å¦‚æœè‡ªåŠ¨ç”Ÿæˆå¤±è´¥ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ›å»ºï¼š

```bash
# 1. åˆ›å»ºç©ºçš„è¿ç§»æ–‡ä»¶
docker compose exec backend alembic revision -m "Initial migration: create all tables"

# 2. ç¼–è¾‘ç”Ÿæˆçš„æ–‡ä»¶ï¼Œæ·»åŠ è¡¨åˆ›å»ºé€»è¾‘
# æ–‡ä»¶ä½ç½®: alembic/versions/xxxxx_initial_migration_create_all_tables.py
```

æŸ¥çœ‹ä¸‹ä¸€èŠ‚çš„ç¤ºä¾‹è¿ç§»è„šæœ¬ã€‚

---

## ç¤ºä¾‹ï¼šåˆå§‹è¿ç§»è„šæœ¬

åˆ›å»ºæ–‡ä»¶: `alembic/versions/001_initial_migration.py`

```python
"""Initial migration: create all tables

Revision ID: 001
Revises: 
Create Date: 2025-11-16 12:00:00.000000

"""
from typing import Sequence, Union
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from pgvector.sqlalchemy import Vector
import uuid

# revision identifiers, used by Alembic.
revision: str = '001'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Enable pgvector extension
    op.execute('CREATE EXTENSION IF NOT EXISTS vector')
    
    # Create users table
    op.create_table('users',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('username', sa.String(length=50), nullable=False),
        sa.Column('email', sa.String(length=100), nullable=False),
        sa.Column('hashed_password', sa.String(length=255), nullable=False),
        sa.Column('is_active', sa.Boolean(), default=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('last_active_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('username'),
        sa.UniqueConstraint('email')
    )
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    op.create_index(op.f('ix_users_username'), 'users', ['username'], unique=True)
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    
    # Create resumes table
    op.create_table('resumes',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('original_filename', sa.String(length=255), nullable=False),
        sa.Column('file_size', sa.Integer(), nullable=True),
        sa.Column('content_type', sa.String(length=100), nullable=False, server_default='application/pdf'),
        sa.Column('s3_key', sa.String(length=500), nullable=False),
        sa.Column('s3_bucket', sa.String(length=100), nullable=False),
        sa.Column('status', sa.String(length=50), nullable=False, server_default='pending'),
        sa.Column('upload_progress', sa.Float(), server_default='0.0'),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.Column('parsed_content', sa.Text(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('summary', sa.Text(), nullable=True),
        sa.Column('skills', postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column('job_titles', postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column('embedding', Vector(1536), nullable=True),
        sa.Column('parsed_at', sa.DateTime(timezone=True), nullable=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_resumes_id'), 'resumes', ['id'], unique=False)
    
    # Create jobs table
    op.create_table('jobs',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('source', sa.String(length=100), nullable=False),
        sa.Column('external_id', sa.String(length=255), nullable=False),
        sa.Column('title', sa.String(length=255), nullable=False),
        sa.Column('company', sa.String(length=255), nullable=True),
        sa.Column('location', sa.String(length=255), nullable=True),
        sa.Column('job_type', sa.String(length=100), nullable=True),
        sa.Column('description', sa.Text(), nullable=True),
        sa.Column('url', sa.String(length=1000), nullable=True),
        sa.Column('posted_date', sa.DateTime(timezone=True), nullable=True),
        sa.Column('salary_min', sa.Integer(), nullable=True),
        sa.Column('salary_max', sa.Integer(), nullable=True),
        sa.Column('salary_currency', sa.String(length=10), nullable=True),
        sa.Column('skills_required', postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column('experience_level', sa.String(length=50), nullable=True),
        sa.Column('embedding', Vector(1536), nullable=True),
        sa.Column('scraped_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('is_active', sa.Boolean(), server_default='true'),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('source', 'external_id', name='uq_job_source_external_id')
    )
    op.create_index(op.f('ix_jobs_id'), 'jobs', ['id'], unique=False)
    op.create_index(op.f('ix_jobs_source'), 'jobs', ['source'], unique=False)
    op.create_index(op.f('ix_jobs_external_id'), 'jobs', ['external_id'], unique=False)
    op.create_index('idx_jobs_embedding', 'jobs', ['embedding'], unique=False, postgresql_using='ivfflat', postgresql_ops={'embedding': 'vector_l2_ops'})
    
    # Create job_matches table
    op.create_table('job_matches',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('resume_id', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('job_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('match_score', sa.Float(), nullable=False),
        sa.Column('ai_analysis', sa.Text(), nullable=True),
        sa.Column('matched_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('user_feedback', sa.String(length=50), nullable=True),
        sa.Column('is_applied', sa.Boolean(), server_default='false'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['resume_id'], ['resumes.id'], ondelete='SET NULL'),
        sa.ForeignKeyConstraint(['job_id'], ['jobs.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('user_id', 'job_id', name='uq_user_job_match')
    )
    op.create_index(op.f('ix_job_matches_id'), 'job_matches', ['id'], unique=False)
    op.create_index(op.f('ix_job_matches_user_id'), 'job_matches', ['user_id'], unique=False)
    op.create_index(op.f('ix_job_matches_match_score'), 'job_matches', ['match_score'], unique=False)


def downgrade() -> None:
    op.drop_index(op.f('ix_job_matches_match_score'), table_name='job_matches')
    op.drop_index(op.f('ix_job_matches_user_id'), table_name='job_matches')
    op.drop_index(op.f('ix_job_matches_id'), table_name='job_matches')
    op.drop_table('job_matches')
    
    op.drop_index('idx_jobs_embedding', table_name='jobs')
    op.drop_index(op.f('ix_jobs_external_id'), table_name='jobs')
    op.drop_index(op.f('ix_jobs_source'), table_name='jobs')
    op.drop_index(op.f('ix_jobs_id'), table_name='jobs')
    op.drop_table('jobs')
    
    op.drop_index(op.f('ix_resumes_id'), table_name='resumes')
    op.drop_table('resumes')
    
    op.drop_index(op.f('ix_users_email'), table_name='users')
    op.drop_index(op.f('ix_users_username'), table_name='users')
    op.drop_index(op.f('ix_users_id'), table_name='users')
    op.drop_table('users')
    
    op.execute('DROP EXTENSION IF EXISTS vector')
```

---

## éƒ¨ç½²åçš„é¦–æ¬¡è¿ç§»æµç¨‹

### å®Œæ•´æ­¥éª¤

```bash
# 1. æœ¬åœ°æäº¤ä»£ç ï¼ˆåŒ…æ‹¬è¿ç§»è„šæœ¬ï¼‰
git add alembic/versions/001_initial_migration.py
git commit -m "feat: add initial database migration"
git push origin main

# 2. GitHub Actions è‡ªåŠ¨éƒ¨ç½²åˆ° EC2

# 3. SSH åˆ° EC2
ssh -i key.pem ubuntu@your-ec2-ip
cd ~/AI-job-matching

# 4. æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker compose ps

# 5. åº”ç”¨è¿ç§»ï¼ˆåˆ›å»ºæ‰€æœ‰è¡¨ï¼‰
docker compose exec backend alembic upgrade head

# 6. éªŒè¯è¡¨å·²åˆ›å»º
docker compose exec backend python -c "
from app.core.database import engine
from sqlalchemy import inspect
inspector = inspect(engine)
print('Tables:', inspector.get_table_names())
"

# åº”è¯¥çœ‹åˆ°:
# Tables: ['users', 'resumes', 'jobs', 'job_matches', 'alembic_version']
```

---

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆ alembic/versions/ æ˜¯ç©ºçš„ï¼Ÿ

A: å› ä¸ºè¿˜æ²¡æœ‰ç”Ÿæˆåˆå§‹è¿ç§»ã€‚è¿™æ˜¯æ­£å¸¸çš„ï¼Œé¦–æ¬¡éƒ¨ç½²æ—¶éœ€è¦ç”Ÿæˆã€‚

### Q: èƒ½å¦åœ¨æœ¬åœ°ç”Ÿæˆåæäº¤ï¼Ÿ

A: **å¯ä»¥**ï¼Œä½†éœ€è¦ï¼š
1. æœ¬åœ°æœ‰ PostgreSQL æ•°æ®åº“
2. `.env` ä¸­çš„ `DATABASE_URL` æŒ‡å‘æœ¬åœ°æ•°æ®åº“
3. å®‰è£…äº† `pgvector` æ‰©å±•

å¦åˆ™å»ºè®®åœ¨ EC2 ä¸Šç”Ÿæˆã€‚

### Q: ç”Ÿæˆè¿ç§»åéœ€è¦ç«‹å³åº”ç”¨å—ï¼Ÿ

A: **é¦–æ¬¡ç”Ÿæˆåå¿…é¡»åº”ç”¨**ï¼Œå¦åˆ™è¡¨ä¸å­˜åœ¨ï¼Œåº”ç”¨æ— æ³•è¿è¡Œã€‚

### Q: å¦‚ä½•å›æ»šè¿ç§»ï¼Ÿ

A: 
```bash
# å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬
docker compose exec backend alembic downgrade -1

# å®Œå…¨å›æ»š
docker compose exec backend alembic downgrade base
```

---

## åç»­å¼€å‘æµç¨‹

ä¿®æ”¹æ¨¡å‹åï¼š

```bash
# 1. ä¿®æ”¹ app/models/*.py
# 2. ç”Ÿæˆæ–°çš„è¿ç§»
docker compose exec backend alembic revision --autogenerate -m "æè¿°å˜æ›´"
# 3. æäº¤ä»£ç 
git add .
git commit -m "feat: æè¿°å˜æ›´"
git push origin main
# 4. éƒ¨ç½²åˆ° EC2 åè¿è¡Œ
docker compose exec backend alembic upgrade head
```

---

## æ€»ç»“

1. âœ… é¦–æ¬¡éƒ¨ç½²å‰/åéœ€è¦ç”Ÿæˆåˆå§‹è¿ç§»è„šæœ¬
2. âœ… å¯ä»¥åœ¨ EC2 ä¸Šç”Ÿæˆï¼Œæˆ–æœ¬åœ°ç”Ÿæˆåæäº¤
3. âœ… ç”Ÿæˆåå¿…é¡»è¿è¡Œ `alembic upgrade head` åˆ›å»ºè¡¨
4. âŒ ä¸éœ€è¦æ‰‹åŠ¨å»ºè¡¨ï¼ŒAlembic ä¼šè‡ªåŠ¨åˆ›å»º
5. âœ… åç»­å¼€å‘æ—¶ï¼Œä¿®æ”¹æ¨¡å‹åç”Ÿæˆæ–°çš„è¿ç§»è„šæœ¬
